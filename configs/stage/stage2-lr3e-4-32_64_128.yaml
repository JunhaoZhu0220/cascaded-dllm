defaults:
  - _self_

work_dir: ${hydra:run.dir}

config_name: stage2_lr3e-4-32_64_128-no-stopword

ngpus: 2
tokens: 50257
baseline: false
stage: 2
# load_dir: /scratch/jz5770/Discrete-Diffusion-Playground/exp_local/openwebtext/stage_2/2025.08.27/050351
# stage1_root_dir: /scratch/jz5770/Discrete-Diffusion-Playground/exp_local/openwebtext/stage_1/2025.08.25/093627

target_lens:
  stage1: 32
  stage2: 64
  stage3: 128

training:
  batch_size: 512
  accum: 1
  n_iters: 500001
  snapshot_freq: 10000
  log_freq: 50
  eval_freq: 100
  snapshot_freq_for_preemption: 10000
  weight: standard
  snapshot_sampling: True
  ema: 0.9999
  enable_conditional_sampling: True  # Whether to perform conditional sampling during training
  enable_unconditional_sampling: False  # Whether to perform unconditional sampling during training

data:
  train: openwebtext
  valid: wikitext103
  cache_dir: data

graph:
  type: absorb
  file: data
  report_all: False

noise:
  type: loglinear
  sigma_min: 1e-4
  sigma_max: 20

sampling:
  predictor: euler
  steps: 128
  noise_removal: True

eval:
  batch_size: 512
  perplexity: True
  perplexity_batch_size: 32

optim:
  weight_decay: 0
  optimizer: AdamW
  lr: 3e-4
  beta1: 0.9
  beta2: 0.999
  eps: 1e-8
  warmup: 2500
  grad_clip: 1.



hydra:
  run:
    dir: exp_local/${data.train}/${config_name}/${now:%Y.%m.%d}/${now:%H%M%S}
  sweep:
    dir: exp/${data.train}/${now:%Y.%m.%d}/${now:%H%M%S}
    subdir: ${hydra.job.num}

wandb:
  project: cascade-discrete-diffusion-keybert
  name: ${data.train}-${model.name}-${config_name}
